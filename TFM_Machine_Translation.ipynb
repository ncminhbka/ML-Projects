{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN07EbiI5cLwcQyrlB/f8uT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncminhbka/ML-Projects/blob/main/TFM_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7aI1UkXTsYL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "url = \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\"\n",
        "if not os.path.exists(\"fra-eng.zip\"):\n",
        "    response = requests.get(url)\n",
        "    with open(\"fra-eng.zip\", \"wb\") as f:\n",
        "        f.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3BukEUp-Ljm",
        "outputId": "2dc65b67-b858-448f-bcff-ffbbac179d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    return eng.lower().strip(), fra.lower().strip()"
      ],
      "metadata": {
        "id": "dv3jzyTV-Phe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "with open(\"fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        eng, fra = normalize(line)\n",
        "        text_pairs.append((eng, fra))"
      ],
      "metadata": {
        "id": "t8QZVR6h_Fym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(text_pairs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90oFGYPv_TZO",
        "outputId": "3ed7ba99-3f13-4484-db0c-077efef5f492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "\n",
        "if os.path.exists(\"en_tokenizer.json\") and os.path.exists(\"fr_tokenizer.json\"):\n",
        "    en_tokenizer = tokenizers.Tokenizer.from_file(\"en_tokenizer.json\")\n",
        "    fr_tokenizer = tokenizers.Tokenizer.from_file(\"fr_tokenizer.json\")\n",
        "else:\n",
        "    en_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
        "    fr_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
        "\n",
        "    # Configure pre-tokenizer to split on whitespace and punctuation, add space at beginning of the sentence\n",
        "    en_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "    fr_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "\n",
        "    # Configure decoder: So that word boundary symbol \"Ġ\" will be removed\n",
        "    en_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "    fr_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "\n",
        "    # Train BPE for English and French using the same trainer\n",
        "    VOCAB_SIZE = 8000\n",
        "    trainer = tokenizers.trainers.BpeTrainer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        special_tokens=[\"[start]\", \"[end]\", \"[pad]\"],\n",
        "        show_progress=True\n",
        "    )\n",
        "    en_tokenizer.train_from_iterator([x[0] for x in text_pairs], trainer=trainer)\n",
        "    fr_tokenizer.train_from_iterator([x[1] for x in text_pairs], trainer=trainer)\n",
        "\n",
        "    en_tokenizer.enable_padding(pad_id=en_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
        "    fr_tokenizer.enable_padding(pad_id=fr_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
        "\n",
        "    # Save the trained tokenizers\n",
        "    en_tokenizer.save(\"en_tokenizer.json\", pretty=True)\n",
        "    fr_tokenizer.save(\"fr_tokenizer.json\", pretty=True)"
      ],
      "metadata": {
        "id": "qB9jm-3v_dBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fr_sample = text_pairs[120][1]"
      ],
      "metadata": {
        "id": "lLmDt_eJA6CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = fr_tokenizer.encode(\"[start] \" + fr_sample + \" [end]\")\n",
        "print(f\"Original: {fr_sample}\")\n",
        "print(f\"Tokens: {encoded.tokens}\")\n",
        "print(f\"IDs: {encoded.ids}\")\n",
        "print(f\"Decoded: {fr_tokenizer.decode(encoded.ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeyDdPWDAnS4",
        "outputId": "005afc8c-3457-49b4-8686-efcadcaa8dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: fous le camp !\n",
            "Tokens: ['[start]', 'Ġfous', 'Ġle', 'Ġcamp', 'Ġ!', 'Ġ', '[end]']\n",
            "IDs: [0, 4169, 127, 2294, 219, 74, 1]\n",
            "Decoded:  fous le camp ! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, text_pairs):\n",
        "        self.text_pairs = text_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng, fra = self.text_pairs[idx]\n",
        "        return eng, \"[start] \" + fra + \" [end]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    en_str, fr_str = zip(*batch)\n",
        "    en_enc = en_tokenizer.encode_batch(en_str, add_special_tokens=True)\n",
        "    fr_enc = fr_tokenizer.encode_batch(fr_str, add_special_tokens=True)\n",
        "    en_ids = [torch.tensor(enc.ids) for enc in en_enc]\n",
        "    fr_ids = [torch.tensor(enc.ids) for enc in fr_enc]\n",
        "\n",
        "    en_pad = pad_sequence(en_ids, batch_first=True, padding_value=en_tokenizer.token_to_id(\"[pad]\"))\n",
        "    fr_pad = pad_sequence(fr_ids, batch_first=True, padding_value=fr_tokenizer.token_to_id(\"[pad]\"))\n",
        "    return en_pad, fr_pad\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "dataset = TranslationDataset(text_pairs)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "bS6iEhqQIJ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_embedding = nn.Embedding(max_len, embedding_dim) #maxlen 768 rat lon de khong lo lookup thieu\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, seq_len = x.size() #seqlen khac maxlen\n",
        "        positions = torch.arange(0, seq_len, dtype=torch.long, device = x.device).expand(bs, seq_len)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "        positional_embeddings = self.positional_embedding(positions)\n",
        "        return token_embeddings + positional_embeddings"
      ],
      "metadata": {
        "id": "Ej_RmPi7A-jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, ffdims, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, ffdims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ffdims, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim, eps = 1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim, eps = 1e-6)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask = None):\n",
        "        attn_output, _ = self.attn(x, x, x, key_padding_mask = mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.norm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "9xZnB3VUCmiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, ffdims, src_vocab_size, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(max_len, src_vocab_size, embedding_dim)\n",
        "        self.encoders = nn.ModuleList([EncoderBlock(embedding_dim, num_heads, ffdims, dropout) for _ in range(num_layers)])\n",
        "    def forward(self, x, mask = None):\n",
        "        x = self.embedding(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x, mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dxrQXyk3EtO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.masked_attn = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_3 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.dropout_3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask):\n",
        "        # 1️⃣ Masked self-attention (decoder nhìn quá khứ)\n",
        "        attn_output, _ = self.masked_attn(\n",
        "            x, x, x,\n",
        "            attn_mask=causal_mask,                # chặn nhìn tương lai\n",
        "            key_padding_mask=tgt_key_padding_mask  # chặn token PAD trong tgt\n",
        "        )\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(x + attn_output)\n",
        "\n",
        "        # 2️⃣ Cross-attention (decoder nhìn encoder output)\n",
        "        attn_output, _ = self.cross_attn(\n",
        "            out_1, enc_output, enc_output, #CHÚ Ý Q K V này\n",
        "            key_padding_mask=src_key_padding_mask  # chặn token PAD trong src\n",
        "        )\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
        "\n",
        "        # 3️⃣ Feed-forward network\n",
        "        ffn_output = self.ffn(out_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
        "\n",
        "        return out_3\n"
      ],
      "metadata": {
        "id": "qlk555w5EiNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, tgt_vocab_size, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(max_len, tgt_vocab_size, embed_dim)\n",
        "        self.decoders = nn.ModuleList([\n",
        "            DecoderBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "    def forward(self, x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask):\n",
        "        x = self.embedding(x)\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ADExpzOrMuiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, embed_dim, num_heads, ff_dim, src_vocab_size, max_len, dropout)\n",
        "        self.decoder = Decoder(num_layers, embed_dim, num_heads, ff_dim, tgt_vocab_size, max_len, dropout)\n",
        "        self.final_layer = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "        self.device = device\n",
        "    def generate_causal_mask(self, seq_len):\n",
        "        if seq_len == 1: # Handle the case where seq_len is 1\n",
        "            return torch.zeros(1, 1, device=self.device)\n",
        "        return torch.triu(torch.ones(seq_len, seq_len, device=self.device) * float('-inf'), diagonal=1)\n",
        "    def generate_key_padding_mask(self, src):\n",
        "        return src == torch.tensor(PAD_ID, device=self.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_key_padding_mask = self.generate_key_padding_mask(src)\n",
        "        tgt_key_padding_mask = self.generate_key_padding_mask(tgt)\n",
        "        causal_mask = self.generate_causal_mask(tgt.size(1)) #tgt: (bs, tgt sqlen), src (bs, src sqlen)\n",
        "        enc_out = self.encoder(src, mask = src_key_padding_mask)\n",
        "        dec_out = self.decoder(tgt, enc_out, src_key_padding_mask = src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, causal_mask = causal_mask)\n",
        "        return self.final_layer(dec_out)"
      ],
      "metadata": {
        "id": "vdjSUTThNi7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer.token_to_id(\"[pad]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F9gJGS1FlmN",
        "outputId": "72e06b0a-b68b-490a-d3c2-19d7125fc668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fr_tokenizer.token_to_id(\"[pad]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gZhxsFHFudl",
        "outputId": "2210cfc6-083e-43f1-c30f-d0ee117a990c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_ID = en_tokenizer.token_to_id(\"[pad]\")"
      ],
      "metadata": {
        "id": "EwLnze8sGEXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 8,\n",
        "    \"ff_dim\": 1024,\n",
        "    \"num_layers\": 4,\n",
        "    \"src_vocab_size\": en_tokenizer.get_vocab_size(),\n",
        "    \"tgt_vocab_size\": fr_tokenizer.get_vocab_size(),\n",
        "    \"max_len\": 256,\n",
        "    \"dropout\": 0.1,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "device = model_config[\"device\"]\n",
        "model = Transformer(**model_config).to(device)"
      ],
      "metadata": {
        "id": "y7sDrsbVG6gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "N_EPOCHS = 60\n",
        "LR = 0.005\n",
        "WARMUP_STEPS = 1000\n",
        "CLIP_NORM = 5.0\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=fr_tokenizer.token_to_id(\"[pad]\"))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=0.01, end_factor=1.0, total_iters=WARMUP_STEPS)\n",
        "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=N_EPOCHS * len(dataloader) - WARMUP_STEPS, eta_min=0)\n",
        "scheduler = optim.lr_scheduler.SequentialLR(\n",
        "    optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[WARMUP_STEPS])"
      ],
      "metadata": {
        "id": "sNn1SBx1IGL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for en_ids, fr_ids in dataloader:\n",
        "        # Move the \"sentences\" to device\n",
        "        en_ids = en_ids.to(device)\n",
        "        fr_ids = fr_ids.to(device)\n",
        "        # zero the grad, then forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(en_ids, fr_ids) #(bs, batchsqlen, tgt vocabsize)\n",
        "        # compute the loss: compare 3D logits to 2D targets\n",
        "        loss = loss_fn(outputs[:, :-1, :].reshape(-1, outputs.shape[-1]), fr_ids[:, 1:].reshape(-1)) #output bỏ cuối, input bỏ đầu\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM, error_if_nonfinite=False)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {epoch_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "8MEg-9mxYL1I",
        "outputId": "e65815de-3984-4245-be1b-684d5d0ebc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60; Avg loss 5.393344523827737\n",
            "Epoch 2/60; Avg loss 5.328317748480046\n",
            "Epoch 3/60; Avg loss 5.294869009982391\n",
            "Epoch 4/60; Avg loss 5.278003667246764\n",
            "Epoch 5/60; Avg loss 5.271035464084311\n",
            "Epoch 6/60; Avg loss 5.2698560476805225\n",
            "Epoch 7/60; Avg loss 5.272284278325507\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3042169281.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#output bỏ cuối, input bỏ đầu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP_NORM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0m_clip_grads_with_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_clip_grads_with_norm_\u001b[0;34m(parameters, max_norm, total_norm, foreach)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for a few samples\n",
        "import random\n",
        "model.eval()\n",
        "N_SAMPLES = 5\n",
        "MAX_LEN = 60\n",
        "with torch.no_grad():\n",
        "    start_token = torch.tensor([fr_tokenizer.token_to_id(\"[start]\")]).to(device) #chỉ có start\n",
        "    for en, true_fr in random.sample(dataset.text_pairs, N_SAMPLES):\n",
        "        en_ids = torch.tensor(en_tokenizer.encode(en).ids).unsqueeze(0).to(device) #lấy ngẫu nhiên 1 câu, chuyển thành id, thêm bs\n",
        "\n",
        "        # get context from encoder\n",
        "        src_mask = model.generate_key_padding_mask(en_ids)\n",
        "        enc_out = model.encoder(en_ids, src_mask)\n",
        "\n",
        "        # generate output from decoder\n",
        "        fr_ids = start_token.unsqueeze(0)\n",
        "        for _ in range(MAX_LEN):\n",
        "            causal_mask = model.generate_causal_mask(fr_ids.size(1))\n",
        "            tgt_mask = model.generate_key_padding_mask(fr_ids)\n",
        "            x = model.decoder(fr_ids, enc_out, src_mask, tgt_mask, causal_mask)\n",
        "            outputs = model.final_layer(x)\n",
        "\n",
        "            outputs = outputs.argmax(dim=-1)\n",
        "            fr_ids = torch.cat([fr_ids, outputs[:, -1:]], axis=-1)\n",
        "            if fr_ids[0, -1] == fr_tokenizer.token_to_id(\"[end]\"):\n",
        "                break\n",
        "\n",
        "        # Decode the predicted IDs\n",
        "        pred_fr = fr_tokenizer.decode(fr_ids[0].tolist())\n",
        "        print(f\"English: {en}\")\n",
        "        print(f\"French: {true_fr}\")\n",
        "        print(f\"Predicted: {pred_fr}\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05sLgFkbfA1F",
        "outputId": "c9ad1660-6dbc-4324-80cb-0d7d2a36595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: he's a total wreck.\n",
            "French: c'est une vraie loque.\n",
            "Predicted:  je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je\n",
            "\n",
            "English: it looks like no one's home.\n",
            "French: on dirait que personne n'est à la maison.\n",
            "Predicted:  je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je\n",
            "\n",
            "English: everybody except me knew what was going on.\n",
            "French: tout le monde sauf moi savait ce qu'il se passait.\n",
            "Predicted:  je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je\n",
            "\n",
            "English: you can't believe anybody.\n",
            "French: vous ne pouvez pas croire à n'importe qui.\n",
            "Predicted:  je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je\n",
            "\n",
            "English: this is a difficult time for all of us.\n",
            "French: c'est un moment difficile pour nous tous.\n",
            "Predicted:  je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je je\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TFM Machine Translation - fixed and optimized\n",
        "\n",
        "Modifications made:\n",
        "- Reduced model size for faster training\n",
        "- Pre-tokenize dataset once (avoid tokenization inside collate_fn)\n",
        "- Fixed tokenizer `add_special_tokens` usage\n",
        "- Added dropout in embedding\n",
        "- Fixed key_padding_mask dtype and shapes\n",
        "- Replaced SequentialLR warmup with Transformer-style LambdaLR\n",
        "- Fixed decoding step to use only last token's logits\n",
        "- Reduced max_len to 128\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import unicodedata\n",
        "import random\n",
        "\n",
        "# Download data if not present\n",
        "url = \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\"\n",
        "if not os.path.exists(\"fra-eng.zip\"):\n",
        "    response = requests.get(url)\n",
        "    with open(\"fra-eng.zip\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# unzip (works in Colab)\n",
        "try:\n",
        "    get_ipython()\n",
        "    !unzip -o fra-eng.zip\n",
        "except Exception:\n",
        "    # Not running in IPython environment — assume already unzipped\n",
        "    pass\n",
        "\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip())\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    return eng.lower().strip(), fra.lower().strip()\n",
        "\n",
        "text_pairs = []\n",
        "with open(\"fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        eng, fra = normalize(line)\n",
        "        text_pairs.append((eng, fra))\n",
        "\n",
        "# Tokenizer setup (train if not exists)\n",
        "import tokenizers\n",
        "\n",
        "if os.path.exists(\"en_tokenizer.json\") and os.path.exists(\"fr_tokenizer.json\"):\n",
        "    en_tokenizer = tokenizers.Tokenizer.from_file(\"en_tokenizer.json\")\n",
        "    fr_tokenizer = tokenizers.Tokenizer.from_file(\"fr_tokenizer.json\")\n",
        "else:\n",
        "    en_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
        "    fr_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
        "\n",
        "    en_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "    fr_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "\n",
        "    en_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "    fr_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "\n",
        "    VOCAB_SIZE = 8000\n",
        "    trainer = tokenizers.trainers.BpeTrainer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        special_tokens=[\"[start]\", \"[end]\", \"[pad]\"],\n",
        "        show_progress=True\n",
        "    )\n",
        "    en_tokenizer.train_from_iterator([x[0] for x in text_pairs], trainer=trainer)\n",
        "    fr_tokenizer.train_from_iterator([x[1] for x in text_pairs], trainer=trainer)\n",
        "\n",
        "    # Enable padding with our [pad] token\n",
        "    en_tokenizer.enable_padding(pad_id=en_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
        "    fr_tokenizer.enable_padding(pad_id=fr_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
        "\n",
        "    en_tokenizer.save(\"en_tokenizer.json\", pretty=True)\n",
        "    fr_tokenizer.save(\"fr_tokenizer.json\", pretty=True)\n",
        "\n",
        "# Quick inspect\n",
        "fr_sample = text_pairs[120][1]\n",
        "encoded = fr_tokenizer.encode(\"[start] \" + fr_sample + \" [end]\")\n",
        "print(\"Example: \", fr_sample)\n",
        "print(\"Tokens (sample):\", encoded.tokens[:20])\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "\n",
        "# Pre-tokenize all data to avoid runtime tokenization overhead\n",
        "PRE_TOKENIZED = []\n",
        "start_id = fr_tokenizer.token_to_id(\"[start]\")\n",
        "end_id = fr_tokenizer.token_to_id(\"[end]\")\n",
        "pad_id = fr_tokenizer.token_to_id(\"[pad]\")\n",
        "\n",
        "for eng, fra in text_pairs:\n",
        "    en_ids = en_tokenizer.encode(eng).ids\n",
        "    fr_ids = [start_id] + fr_tokenizer.encode(fra).ids + [end_id]\n",
        "    PRE_TOKENIZED.append((torch.tensor(en_ids, dtype=torch.long), torch.tensor(fr_ids, dtype=torch.long)))\n",
        "\n",
        "class EncodedTranslationDataset(Dataset):\n",
        "    def __init__(self, encoded_pairs):\n",
        "        self.pairs = encoded_pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "# collate only pads tensors\n",
        "def collate_fn(batch):\n",
        "    en_ids, fr_ids = zip(*batch)\n",
        "    en_pad = pad_sequence(en_ids, batch_first=True, padding_value=en_tokenizer.token_to_id(\"[pad]\"))\n",
        "    fr_pad = pad_sequence(fr_ids, batch_first=True, padding_value=fr_tokenizer.token_to_id(\"[pad]\"))\n",
        "    return en_pad, fr_pad\n",
        "\n",
        "# Smaller model config for faster experiments\n",
        "BATCH_SIZE = 32\n",
        "dataset = EncodedTranslationDataset(PRE_TOKENIZED)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Transformer implementation (slimmed and fixes)\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, vocab_size, embedding_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_embedding = nn.Embedding(max_len, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, seq_len = x.size()\n",
        "        positions = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(bs, seq_len)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "        positional_embeddings = self.positional_embedding(positions)\n",
        "        return self.dropout(token_embeddings + positional_embeddings)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, ffdims, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, ffdims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ffdims, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        # key_padding_mask: (batch, seq) bool where True means position is PAD and should be ignored\n",
        "        attn_output, _ = self.attn(x, x, x, key_padding_mask=key_padding_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, ffdims, src_vocab_size, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(max_len, src_vocab_size, embedding_dim, dropout)\n",
        "        self.encoders = nn.ModuleList([EncoderBlock(embedding_dim, num_heads, ffdims, dropout) for _ in range(num_layers)])\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x, key_padding_mask=key_padding_mask)\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.masked_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_3 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.dropout_3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask):\n",
        "        # Masked self-attention (prevent looking at future positions)\n",
        "        attn_output, _ = self.masked_attn(\n",
        "            x, x, x,\n",
        "            attn_mask=causal_mask,                # (tgt_len, tgt_len) additive mask\n",
        "            key_padding_mask=tgt_key_padding_mask  # (batch, tgt_len) bool\n",
        "        )\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(x + attn_output)\n",
        "\n",
        "        # Cross-attention\n",
        "        attn_output, _ = self.cross_attn(\n",
        "            out_1, enc_output, enc_output,\n",
        "            key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(out_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
        "\n",
        "        return out_3\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, tgt_vocab_size, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(max_len, tgt_vocab_size, embed_dim, dropout)\n",
        "        self.decoders = nn.ModuleList([DecoderBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "    def forward(self, x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask):\n",
        "        x = self.embedding(x)\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x, enc_output, src_key_padding_mask, tgt_key_padding_mask, causal_mask)\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, embed_dim, num_heads, ff_dim, src_vocab_size, max_len, dropout)\n",
        "        self.decoder = Decoder(num_layers, embed_dim, num_heads, ff_dim, tgt_vocab_size, max_len, dropout)\n",
        "        self.final_layer = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "        self.device = device\n",
        "\n",
        "    def generate_causal_mask(self, seq_len):\n",
        "        # Returns an additive mask for MultiheadAttention: shape (tgt_len, tgt_len)\n",
        "        if seq_len == 1:\n",
        "            return torch.zeros((1, 1), device=self.device)\n",
        "        mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=self.device), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def generate_key_padding_mask(self, src):\n",
        "        # src: (batch, seq)\n",
        "        return (src == pad_id).to(torch.bool)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_key_padding_mask = self.generate_key_padding_mask(src)  # (batch, src_len)\n",
        "        tgt_key_padding_mask = self.generate_key_padding_mask(tgt)  # (batch, tgt_len)\n",
        "        causal_mask = self.generate_causal_mask(tgt.size(1))       # (tgt_len, tgt_len)\n",
        "        enc_out = self.encoder(src, key_padding_mask=src_key_padding_mask)\n",
        "        dec_out = self.decoder(tgt, enc_out, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, causal_mask=causal_mask)\n",
        "        return self.final_layer(dec_out)\n",
        "\n",
        "# Model config (reduced for faster training)\n",
        "model_config = {\n",
        "    \"embed_dim\": 256,\n",
        "    \"num_heads\": 4,\n",
        "    \"ff_dim\": 1024,\n",
        "    \"num_layers\": 3,\n",
        "    \"src_vocab_size\": en_tokenizer.get_vocab_size(),\n",
        "    \"tgt_vocab_size\": fr_tokenizer.get_vocab_size(),\n",
        "    \"max_len\": 128,\n",
        "    \"dropout\": 0.1,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "device = model_config[\"device\"]\n",
        "model = Transformer(**model_config).to(device)\n",
        "\n",
        "# Training setup\n",
        "import torch.optim as optim\n",
        "N_EPOCHS = 20\n",
        "WARMUP_STEPS = 4000\n",
        "CLIP_NORM = 1.0\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "# Use Adam with lr=1.0 and scale using LambdaLR (Transformer warmup schedule)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1.0)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    # step is 0-indexed in LambdaLR; convert to 1-indexed for formula\n",
        "    d_model = model_config['embed_dim']\n",
        "    step = max(step + 1, 1)\n",
        "    return (d_model ** -0.5) * min(step ** -0.5, step * (WARMUP_STEPS ** -1.5))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for en_ids, fr_ids in dataloader:\n",
        "        en_ids = en_ids.to(device)\n",
        "        fr_ids = fr_ids.to(device)\n",
        "\n",
        "        # decoder input is fr_ids[:, :-1], target is fr_ids[:, 1:]\n",
        "        decoder_input = fr_ids[:, :-1]\n",
        "        decoder_target = fr_ids[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(en_ids, decoder_input)  # (bs, tgt_len-1, vocab)\n",
        "\n",
        "        loss = loss_fn(outputs.reshape(-1, outputs.shape[-1]), decoder_target.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {avg_loss:.4f}\")\n",
        "\n",
        "# Simple greedy decoding for evaluation on a few samples\n",
        "model.eval()\n",
        "N_SAMPLES = 5\n",
        "MAX_LEN = 60\n",
        "with torch.no_grad():\n",
        "    start_token = torch.tensor([start_id]).to(device)\n",
        "    for en_ids, true_fr_ids in random.sample(PRE_TOKENIZED, N_SAMPLES):\n",
        "        en_ids = en_ids.unsqueeze(0).to(device)\n",
        "        src_mask = model.generate_key_padding_mask(en_ids)\n",
        "        enc_out = model.encoder(en_ids, src_mask)\n",
        "\n",
        "        fr_ids = start_token.unsqueeze(0)  # shape (1,1)\n",
        "        for _ in range(MAX_LEN):\n",
        "            causal_mask = model.generate_causal_mask(fr_ids.size(1))\n",
        "            tgt_mask = model.generate_key_padding_mask(fr_ids)\n",
        "            x = model.decoder(fr_ids, enc_out, src_mask, tgt_mask, causal_mask)\n",
        "            logits = model.final_layer(x)  # (1, seq_len, vocab)\n",
        "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (1,1)\n",
        "            fr_ids = torch.cat([fr_ids, next_token], dim=-1)\n",
        "            if fr_ids[0, -1].item() == end_id:\n",
        "                break\n",
        "\n",
        "        pred_ids = fr_ids[0].tolist()\n",
        "        pred_text = fr_tokenizer.decode(pred_ids)\n",
        "        true_text = fr_tokenizer.decode(true_fr_ids.tolist())\n",
        "        eng_text = en_tokenizer.decode(en_ids[0].tolist())\n",
        "\n",
        "        print(\"English:\", eng_text)\n",
        "        print(\"True French:\", true_text)\n",
        "        print(\"Predicted:\", pred_text)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn6KBGz-0AD0",
        "outputId": "f2d6b16e-4862-4861-9221-4ca543db40a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "Example:  fous le camp !\n",
            "Tokens (sample): ['[start]', 'Ġfous', 'Ġle', 'Ġcamp', 'Ġ!', 'Ġ', '[end]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20; Avg loss 2.9733\n",
            "Epoch 2/20; Avg loss 1.5412\n",
            "Epoch 3/20; Avg loss 1.1615\n",
            "Epoch 4/20; Avg loss 0.9641\n",
            "Epoch 5/20; Avg loss 0.8381\n",
            "Epoch 6/20; Avg loss 0.7467\n",
            "Epoch 7/20; Avg loss 0.6764\n",
            "Epoch 8/20; Avg loss 0.6201\n",
            "Epoch 9/20; Avg loss 0.5743\n",
            "Epoch 10/20; Avg loss 0.5360\n",
            "Epoch 11/20; Avg loss 0.5018\n",
            "Epoch 12/20; Avg loss 0.4736\n",
            "Epoch 13/20; Avg loss 0.4489\n",
            "Epoch 14/20; Avg loss 0.4257\n",
            "Epoch 15/20; Avg loss 0.4063\n",
            "Epoch 16/20; Avg loss 0.3889\n",
            "Epoch 17/20; Avg loss 0.3726\n",
            "Epoch 18/20; Avg loss 0.3589\n",
            "Epoch 19/20; Avg loss 0.3466\n",
            "Epoch 20/20; Avg loss 0.3340\n",
            "English:  she's drop-dead gorgeous.\n",
            "True French:  elle est bonne à tomber raide.\n",
            "Predicted:  elle est bonne à tomber.\n",
            "\n",
            "English:  you can't judge a book by its cover.\n",
            "True French:  on ne doit pas juger un livre sur sa reliure.\n",
            "Predicted:  on ne peut pas juger un livre sur sa couverture.\n",
            "\n",
            "English:  he is probably still alive.\n",
            "True French:  il est probablement toujours vivant.\n",
            "Predicted:  il est probablement encore en vie.\n",
            "\n",
            "English:  i take a bath every day.\n",
            "True French:  je me baigne tous les jours.\n",
            "Predicted:  je me baigne tous les jours.\n",
            "\n",
            "English:  tell me the reason you were absent from school yesterday.\n",
            "True French:  dites-moi la raison pour laquelle vous étiez absents de l'école hier.\n",
            "Predicted:  dis-moi la raison pour laquelle tu étais absent de l'école hier.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple greedy decoding for evaluation on a few samples\n",
        "model.eval()\n",
        "N_SAMPLES = 5\n",
        "MAX_LEN = 60\n",
        "with torch.no_grad():\n",
        "    start_token = torch.tensor([start_id]).to(device)\n",
        "    for en_ids, true_fr_ids in random.sample(PRE_TOKENIZED, N_SAMPLES):\n",
        "        en_ids = en_ids.unsqueeze(0).to(device)\n",
        "        src_mask = model.generate_key_padding_mask(en_ids)\n",
        "        enc_out = model.encoder(en_ids, src_mask)\n",
        "\n",
        "        fr_ids = start_token.unsqueeze(0)  # shape (1,1)\n",
        "        for _ in range(MAX_LEN):\n",
        "            causal_mask = model.generate_causal_mask(fr_ids.size(1))\n",
        "            tgt_mask = model.generate_key_padding_mask(fr_ids)\n",
        "            x = model.decoder(fr_ids, enc_out, src_mask, tgt_mask, causal_mask)\n",
        "            logits = model.final_layer(x)  # (1, seq_len, vocab)\n",
        "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (1,1)\n",
        "            fr_ids = torch.cat([fr_ids, next_token], dim=-1)\n",
        "            if fr_ids[0, -1].item() == end_id:\n",
        "                break\n",
        "\n",
        "        pred_ids = fr_ids[0].tolist()\n",
        "        pred_text = fr_tokenizer.decode(pred_ids)\n",
        "        true_text = fr_tokenizer.decode(true_fr_ids.tolist())\n",
        "        eng_text = en_tokenizer.decode(en_ids[0].tolist())\n",
        "\n",
        "        print(\"English:\", eng_text)\n",
        "        print(\"True French:\", true_text)\n",
        "        print(\"Predicted:\", pred_text)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzcVHWQD4eLf",
        "outputId": "8a775396-94dd-487b-8b03-c40d8f9f9030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  i want this building locked.\n",
            "True French:  je veux que cet immeuble soit verrouillé.\n",
            "Predicted:  je veux que cet immeuble soit verrouillé.\n",
            "\n",
            "English:  i think it's time for me to abandon that plan.\n",
            "True French:  je pense qu'il est temps pour moi de laisser tomber ce projet.\n",
            "Predicted:  je pense qu'il est temps pour moi d'abandonner ce projet.\n",
            "\n",
            "English:  i need to talk to you about an urgent matter.\n",
            "True French:  il me faut vous entretenir d'une affaire pressante.\n",
            "Predicted:  il me faut vous entretenir d'une affaire pressante.\n",
            "\n",
            "English:  haven't you caused enough trouble already?\n",
            "True French:  t'as pas causé assez de problèmes comme ça ?\n",
            "Predicted:  n'as-tu pas assez causé de problèmes ?\n",
            "\n",
            "English:  don't you want to see the world?\n",
            "True French:  ne veux-tu pas voir le monde ?\n",
            "Predicted:  ne voulez-vous pas voir le monde ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def translate_sentence(model, en_tokenizer, fr_tokenizer, sentence, max_len=60, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_id = fr_tokenizer.token_to_id(\"[start]\")\n",
        "        end_id = fr_tokenizer.token_to_id(\"[end]\")\n",
        "\n",
        "        # Encode English input\n",
        "        en_ids = torch.tensor(en_tokenizer.encode(sentence).ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        src_mask = model.generate_key_padding_mask(en_ids)\n",
        "        enc_out = model.encoder(en_ids, src_mask)\n",
        "\n",
        "        # Start decoding\n",
        "        fr_ids = torch.tensor([[start_id]], dtype=torch.long, device=device)\n",
        "        for _ in range(max_len):\n",
        "            causal_mask = model.generate_causal_mask(fr_ids.size(1))\n",
        "            tgt_mask = model.generate_key_padding_mask(fr_ids)\n",
        "            x = model.decoder(fr_ids, enc_out, src_mask, tgt_mask, causal_mask)\n",
        "            logits = model.final_layer(x)\n",
        "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "            fr_ids = torch.cat([fr_ids, next_token], dim=-1)\n",
        "            if fr_ids[0, -1].item() == end_id:\n",
        "                break\n",
        "\n",
        "        pred_text = fr_tokenizer.decode(fr_ids[0].tolist())\n",
        "        return pred_text\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "input = \"Excuse me, where is the nearest coffee shop?\"\n",
        "result = translate_sentence(model, en_tokenizer, fr_tokenizer, input, device=device)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMB8404E5jWQ",
        "outputId": "9f4c8118-270d-4a9c-88b2-f49f87ef6a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " it-moi quand est le café le plus proche ?\n"
          ]
        }
      ]
    }
  ]
}